{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "### Camilo Montenegro\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import MapType, StringType, StructType, IntegerType, DoubleType, TimestampType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, split\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "This capstone project is intended to cover the topics learned during this Nanodegree, and execute some final analysis over the data set so an analytics dashboard can be created in a further iteration. \n",
    "\n",
    "The results obtained from this data discovery can be useful to understand how the Tennis world has been moving during the years. So this data set analysis can be used by various actors such as national and world tennis federations, trainers, tennis players, and fans. They can also discover how was the performance of a player in some or specific tournaments, how that player had been developing his/her skills based on match results, and lots more of analysis. This project easily can be extended to a web app dashboard in which fans can execute user-friendly queries and discover new information.\n",
    "\n",
    "Here we made use of some tools such as Spark and Pandas in a Python > 3.x environment\n",
    "\n",
    "The project's plan is defined in the next steps:\n",
    "1. Gather the required information, in this cause we're using an ATP Tennis Data Set that will be explained in the upcoming section\n",
    "2. Analyze the documentation of the data to understand which analysis can be performed using the existing data\n",
    "3. Extract the information into the current spark session, so the following data processing steps can be execute\n",
    "4. Perform data cleaning removing unwanted data such as null, empty and duplicates tuples\n",
    "5. Create the data pipeline in which spark is in charge of creating the tables based on the previous information\n",
    "6. Run some quality checks after the pipeline is performed to be sure that the data is consistent\n",
    "7. Perform some analytics query to show the potential of using spark and the created data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "\n",
    "- The data set used in this project was extracted and loaded from this Github repository: https://github.com/JeffSackmann/tennis_atp.\n",
    "\n",
    "- This contains master ATP player file, historical rankings, results, and match stats.\n",
    "\n",
    "- The player file columns are player_id, first_name, last_name, hand, birth_date, country_code.\n",
    "\n",
    "- The columns for the ranking files are ranking_date, ranking, player_id, ranking_points (where available).\n",
    "\n",
    "- ATP rankings are mostly complete from 1985 to the present. 1982 is missing, and rankings from 1973-1984 are only intermittent.\n",
    "\n",
    "- Results and stats: There are up to three files per season: One for tour-level main draw matches (e.g. 'atp_matches_2014.csv'), one for tour-level qualifying and challenger main-draw matches, and one for futures matches.\n",
    "\n",
    "- Most of the columns in the results files are self-explanatory. There's also included a matches_data_dictionary.txt file to spell things out a bit more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df = spark.read.load(\"atpTennisData/tennis_atp/atp_players.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\").toDF(\n",
    "  'playerId','firstName','lastName','hand','bornDate','country'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ranking_date: string (nullable = true)\n",
      " |-- rank: string (nullable = true)\n",
      " |-- player: string (nullable = true)\n",
      " |-- points: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54938"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------+----+--------+-------+\n",
      "|playerId|     firstName|lastName|hand|bornDate|country|\n",
      "+--------+--------------+--------+----+--------+-------+\n",
      "|  100001|       Gardnar|  Mulloy|   R|19131122|    USA|\n",
      "|  100002|        Pancho|  Segura|   R|19210620|    ECU|\n",
      "|  100003|         Frank| Sedgman|   R|19271002|    AUS|\n",
      "|  100004|      Giuseppe|   Merlo|   R|19271011|    ITA|\n",
      "|  100005|Richard Pancho|Gonzales|   R|19280509|    USA|\n",
      "+--------+--------------+--------+----+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df.write.partitionBy('country','playerId').format('parquet').save('atpTennisData/spark/atp_playersV3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=spark.read.parquet(\"atpTennisData/spark/atp_playersV3.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore & Clean the Data \n",
    "After reading all the information using spark environment, it was necesary to understand the schema that we were working on. So for all the gather information the data schema was discovered. Afterwards, we perfomed the spark describe function to identify the behaviour of the data (count, mean, stddev, min, max) and missing values. With this in mind, and for each table we dropped the tuples that had one or more missing values.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playerId: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- hand: string (nullable = true)\n",
      " |-- bornDate: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54938"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------+----+--------+-------+\n",
      "|playerId|     firstName|lastName|hand|bornDate|country|\n",
      "+--------+--------------+--------+----+--------+-------+\n",
      "|  100001|       Gardnar|  Mulloy|   R|19131122|    USA|\n",
      "|  100002|        Pancho|  Segura|   R|19210620|    ECU|\n",
      "|  100003|         Frank| Sedgman|   R|19271002|    AUS|\n",
      "|  100004|      Giuseppe|   Merlo|   R|19271011|    ITA|\n",
      "|  100005|Richard Pancho|Gonzales|   R|19280509|    USA|\n",
      "+--------+--------------+--------+----+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+--------+-----+--------------------+-------+\n",
      "|summary|         playerId|     firstName|lastName| hand|            bornDate|country|\n",
      "+-------+-----------------+--------------+--------+-----+--------------------+-------+\n",
      "|  count|            54938|         54761|   54896|49152|               43356|  54883|\n",
      "|   mean|137411.8036331865|          null|     1.0| null|1.9816213222206846E7|   null|\n",
      "| stddev|33843.15964803341|          null|     NaN| null|   321641.3326366638|   null|\n",
      "|    min|           100001|\"Jason \"\"Jj\"\"\"|       1|    A|              185903|    AFG|\n",
      "|    max|           209903|       Zygmunt|  Zysset|    U|            20050210|    ZIM|\n",
      "+-------+-----------------+--------------+--------+-----+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid = df.dropna(how = \"any\", subset = [\"firstName\", \"lastName\",\"hand\",\"bornDate\",\"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42055"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playerId: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- hand: string (nullable = true)\n",
      " |-- bornDate: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- ts_bornDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------+----+--------+-------+-------------------+\n",
      "|playerId|     firstName|lastName|hand|bornDate|country|        ts_bornDate|\n",
      "+--------+--------------+--------+----+--------+-------+-------------------+\n",
      "|  100001|       Gardnar|  Mulloy|   R|19131122|    USA|1913-11-22 00:00:00|\n",
      "|  100002|        Pancho|  Segura|   R|19210620|    ECU|1921-06-20 00:00:00|\n",
      "|  100003|         Frank| Sedgman|   R|19271002|    AUS|1927-10-02 00:00:00|\n",
      "|  100004|      Giuseppe|   Merlo|   R|19271011|    ITA|1927-10-11 00:00:00|\n",
      "|  100005|Richard Pancho|Gonzales|   R|19280509|    USA|1928-05-09 00:00:00|\n",
      "+--------+--------------+--------+----+--------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_valid.show(5,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "df_spark_ranks = spark.read.load(\"atpTennisData/tennis_atp/atp_rankings_*.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ranking_date: integer (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      " |-- player: integer (nullable = true)\n",
      " |-- points: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_ranks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2857079"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_ranks.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+------+\n",
      "|ranking_date|rank|player|points|\n",
      "+------------+----+------+------+\n",
      "|    20000110|   1|101736|  4135|\n",
      "|    20000110|   2|102338|  2915|\n",
      "|    20000110|   3|101948|  2419|\n",
      "|    20000110|   4|103017|  2184|\n",
      "|    20000110|   5|102856|  2169|\n",
      "+------------+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_ranks.show(5,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+------------------+\n",
      "|summary|        ranking_date|              rank|            player|            points|\n",
      "+-------+--------------------+------------------+------------------+------------------+\n",
      "|  count|             2857079|           2857079|           2857079|           2509664|\n",
      "|   mean|2.0036640241374496E7| 827.7547946696609|109680.05734108157|114.90980426065003|\n",
      "| stddev|    97916.8959197867|506.91610232538534|17974.363400013513| 416.5161568367033|\n",
      "|    min|            19730827|                 1|            100001|                 1|\n",
      "|    max|            20200309|              2271|            209866|             16950|\n",
      "+-------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_ranks.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid_ranks = df_spark_ranks.dropna(how = \"any\", subset = [\"points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+------------------+\n",
      "|summary|       ranking_date|             rank|            player|            points|\n",
      "+-------+-------------------+-----------------+------------------+------------------+\n",
      "|  count|            2509664|          2509664|           2509664|           2509664|\n",
      "|   mean|2.005838226890014E7|866.7871468053094|110556.60943138205|114.90980426065003|\n",
      "| stddev|  82402.91547965597|515.6314137517035|18872.154068893873| 416.5161568367033|\n",
      "|    min|           19900101|                1|            100019|                 1|\n",
      "|    max|           20200309|             2271|            209866|             16950|\n",
      "+-------+-------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_valid_ranks.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid_ranks = df_spark_valid_ranks.withColumn(\"ts_rankingDate\", toTimeStampFormat(\"ranking_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ranking_date: integer (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      " |-- player: integer (nullable = true)\n",
      " |-- points: integer (nullable = true)\n",
      " |-- ts_rankingDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_valid_ranks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid_ranks.show(5,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_matches = spark.read.load(\"atpTennisData/tennis_atp/atp_matches_*.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tourney_id: string (nullable = true)\n",
      " |-- tourney_name: string (nullable = true)\n",
      " |-- surface: string (nullable = true)\n",
      " |-- draw_size: string (nullable = true)\n",
      " |-- tourney_level: string (nullable = true)\n",
      " |-- tourney_date: string (nullable = true)\n",
      " |-- match_num: string (nullable = true)\n",
      " |-- winner_id: string (nullable = true)\n",
      " |-- winner_seed: string (nullable = true)\n",
      " |-- winner_entry: string (nullable = true)\n",
      " |-- winner_name: string (nullable = true)\n",
      " |-- winner_hand: string (nullable = true)\n",
      " |-- winner_ht: string (nullable = true)\n",
      " |-- winner_ioc: string (nullable = true)\n",
      " |-- winner_age: string (nullable = true)\n",
      " |-- loser_id: string (nullable = true)\n",
      " |-- loser_seed: string (nullable = true)\n",
      " |-- loser_entry: string (nullable = true)\n",
      " |-- loser_name: string (nullable = true)\n",
      " |-- loser_hand: string (nullable = true)\n",
      " |-- loser_ht: string (nullable = true)\n",
      " |-- loser_ioc: string (nullable = true)\n",
      " |-- loser_age: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- best_of: string (nullable = true)\n",
      " |-- round: string (nullable = true)\n",
      " |-- minutes: string (nullable = true)\n",
      " |-- w_ace: string (nullable = true)\n",
      " |-- w_df: string (nullable = true)\n",
      " |-- w_svpt: string (nullable = true)\n",
      " |-- w_1stIn: string (nullable = true)\n",
      " |-- w_1stWon: string (nullable = true)\n",
      " |-- w_2ndWon: string (nullable = true)\n",
      " |-- w_SvGms: string (nullable = true)\n",
      " |-- w_bpSaved: string (nullable = true)\n",
      " |-- w_bpFaced: string (nullable = true)\n",
      " |-- l_ace: string (nullable = true)\n",
      " |-- l_df: string (nullable = true)\n",
      " |-- l_svpt: string (nullable = true)\n",
      " |-- l_1stIn: string (nullable = true)\n",
      " |-- l_1stWon: string (nullable = true)\n",
      " |-- l_2ndWon: string (nullable = true)\n",
      " |-- l_SvGms: string (nullable = true)\n",
      " |-- l_bpSaved: string (nullable = true)\n",
      " |-- l_bpFaced: string (nullable = true)\n",
      " |-- winner_rank: string (nullable = true)\n",
      " |-- winner_rank_points: string (nullable = true)\n",
      " |-- loser_rank: string (nullable = true)\n",
      " |-- loser_rank_points: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_matches.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['winner_seed','winner_entry','winner_ioc','loser_seed','loser_entry','w_ace','l_ace','w_svpt','l_svpt','w_1stIn','l_1stIn','w_SvGms','lSvGms','w_bpSaved','w_bpFaced','l_bpSaved','l_bpFaced']\n",
    "df_spark_matches = df_spark_matches.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tourney_id: string (nullable = true)\n",
      " |-- tourney_name: string (nullable = true)\n",
      " |-- surface: string (nullable = true)\n",
      " |-- draw_size: string (nullable = true)\n",
      " |-- tourney_level: string (nullable = true)\n",
      " |-- tourney_date: string (nullable = true)\n",
      " |-- match_num: string (nullable = true)\n",
      " |-- winner_id: string (nullable = true)\n",
      " |-- winner_name: string (nullable = true)\n",
      " |-- winner_hand: string (nullable = true)\n",
      " |-- winner_ht: string (nullable = true)\n",
      " |-- winner_age: string (nullable = true)\n",
      " |-- loser_id: string (nullable = true)\n",
      " |-- loser_name: string (nullable = true)\n",
      " |-- loser_hand: string (nullable = true)\n",
      " |-- loser_ht: string (nullable = true)\n",
      " |-- loser_ioc: string (nullable = true)\n",
      " |-- loser_age: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- best_of: string (nullable = true)\n",
      " |-- round: string (nullable = true)\n",
      " |-- minutes: string (nullable = true)\n",
      " |-- w_df: string (nullable = true)\n",
      " |-- w_1stWon: string (nullable = true)\n",
      " |-- w_2ndWon: string (nullable = true)\n",
      " |-- l_df: string (nullable = true)\n",
      " |-- l_1stWon: string (nullable = true)\n",
      " |-- l_2ndWon: string (nullable = true)\n",
      " |-- l_SvGms: string (nullable = true)\n",
      " |-- winner_rank: string (nullable = true)\n",
      " |-- winner_rank_points: string (nullable = true)\n",
      " |-- loser_rank: string (nullable = true)\n",
      " |-- loser_rank_points: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_matches.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814534"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_matches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+-------+------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------+------------------+---------+-----------------+------------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|tourney_id|    tourney_name|surface|         draw_size|     tourney_level|        tourney_date|         match_num|         winner_id|       winner_name|       winner_hand|         winner_ht|        winner_age|          loser_id|        loser_name|loser_hand|          loser_ht|loser_ioc|        loser_age|       score|           best_of|             round|          minutes|             w_df|          w_1stWon|          w_2ndWon|             l_df|          l_1stWon|         l_2ndWon|           l_SvGms|       winner_rank|winner_rank_points|       loser_rank| loser_rank_points|\n",
      "+-------+----------+----------------+-------+------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------+------------------+---------+-----------------+------------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|    814534|          814534| 811977|            814474|            814534|              814534|            814534|            814534|            790409|            814251|            423729|            781964|            814532|            814534|    814025|            343753|   814479|           793910|      814273|            814534|            811459|           182302|           187765|            187765|            187752|           187738|            187406|           187406|            187131|            730512|            654975|           654484|            602548|\n",
      "|   mean|      null|            null|   null| 38.80227729798619|18.011764705882353|2.0036725237890624E7| 71.93616841040398|107305.03749137543|              null|104488.57774915716| 6686.488248385171|23.934386242014153|109788.38899300247|              null|      null| 184.0107548152307|     null| 23.7294272464331|        null|3.1000272795904253|186.22868167202571|98.42094764052007|2.671087122221465|33.726828240514095|17.572538634928424|7.021845544735081|36.969819536194144|315.8812684759293|28.802924154736523| 483.0227100992181| 297.6038108324745|550.4727510527377|207.37211475268361|\n",
      "| stddev|      null|            null|   null|22.474726240318947| 4.588592087723431|  117863.60051994109|173.59381863369643|15224.023105171289|              null|12736.877467731549|25456.602184357344|3.8667741217162215|19130.444783110313|              null|      null|6.6376800157023474|     null|4.222973835330896|        null|0.4359450364861128|6.3268604857895765|38.52338254874224|2.283885381522193|12.358707321805088| 7.746260264922409|9.295746784312136|  54.4728100770958|1099.181654292495| 86.88949399266274|507.01416993239275| 798.1124584327928|447.5254839542156| 485.0959951826328|\n",
      "|    min| 1968-2029|'s-Hertogenbosch| Carpet|                10|                15|            19680119|                 1|            100002|        Sulistyono|            100431|                 0|              1 AL|           0-1 RET|                  |         A|                 0|      AHO|     14.006844627| 1-6 0-3 6-6|                 3|               163|                0|                0|                 0|                 0|                0|                 0|                0|                 0|                 0|                 0|                0|                 0|\n",
      "|    max| 2020-M056|     thailand F2|   None|                96|                 S|            20200309|                99|            209902|Zvonimir Oreskovic|                 U|            900027|                WC|               W/O|Zvonimir Oreskovic|         U|               208|      ZIM|    65.7522245038|      Zhe Li|                 U|                SF|              ZIM|     Zhuoyang Qiu|               ZIM|                 9|                9|               996|             9990|               997|               999|               999|              999|               999|\n",
      "+-------+----------+----------------+-------+------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------+------------------+---------+-----------------+------------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_matches.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------+---------+-------------+------------+---------+---------+-----------------+-----------+---------+-------------+--------+------------------+----------+--------+---------+-------------+----------+-------+-----+-------+----+--------+--------+----+--------+--------+-------+-----------+------------------+----------+-----------------+\n",
      "|          tourney_id|tourney_name|surface|draw_size|tourney_level|tourney_date|match_num|winner_id|      winner_name|winner_hand|winner_ht|   winner_age|loser_id|        loser_name|loser_hand|loser_ht|loser_ioc|    loser_age|     score|best_of|round|minutes|w_df|w_1stWon|w_2ndWon|l_df|l_1stWon|l_2ndWon|l_SvGms|winner_rank|winner_rank_points|loser_rank|loser_rank_points|\n",
      "+--------------------+------------+-------+---------+-------------+------------+---------+---------+-----------------+-----------+---------+-------------+--------+------------------+----------+--------+---------+-------------+----------+-------+-----+-------+----+--------+--------+----+--------+--------+-------+-----------+------------------+----------+-----------------+\n",
      "|2015-M-FU-GER-01A...|  Germany F1| Carpet|       32|            S|    20150105|        1|   105590|Uladzimir Ignatik|          R|     null|24.4791238877|  131867|   Dominik Boehler|         R|    null|      GER|18.8720054757|7-6(5) 6-3|      3|  R32|   null|null|    null|    null|null|    null|    null|   null|        218|               223|      2159|                1|\n",
      "|2015-M-FU-GER-01A...|  Germany F1| Carpet|       32|            S|    20150105|        2|   123828|     Jan Choinski|          R|     null|18.5708418891|  127157|   Daniel Altmaier|         U|    null|      GER|16.3148528405|7-5 7-6(3)|      3|  R32|   null|null|    null|    null|null|    null|    null|   null|        661|                38|       984|               12|\n",
      "|2015-M-FU-GER-01A...|  Germany F1| Carpet|       32|            S|    20150105|        3|   105786|  Tom Schonenberg|          R|     null|23.5592060233|  105617|      Andreas Mies|         U|    null|      GER|24.3750855578|7-6(2) 6-4|      3|  R32|   null|null|    null|    null|null|    null|    null|   null|        833|                20|       922|               15|\n",
      "|2015-M-FU-GER-01A...|  Germany F1| Carpet|       32|            S|    20150105|        4|   122351|Johannes Haerteis|          U|     null|18.8692676249|  106154|        Duje Kekez|         R|    null|      CRO|21.7686516085|   6-4 6-3|      3|  R32|   null|null|    null|    null|null|    null|    null|   null|        794|                22|       493|               72|\n",
      "|2015-M-FU-GER-01A...|  Germany F1| Carpet|       32|            S|    20150105|        5|   134068|       Vadym Ursu|          R|     null|18.0095824778|  122171|Alexander Mannapov|         R|    null|      GER|19.7097878166|   6-4 6-4|      3|  R32|   null|null|    null|    null|null|    null|    null|   null|        985|                12|      2122|                1|\n",
      "+--------------------+------------+-------+---------+-------------+------------+---------+---------+-----------------+-----------+---------+-------------+--------+------------------+----------+--------+---------+-------------+----------+-------+-----+-------+----+--------+--------+----+--------+--------+-------+-----------+------------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_matches.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "![Hola](./images/ConceptualDataModel.png)\n",
    "\n",
    "As you can see, the proposed Conceptual Data Model is similar to a Star Schema in the sense that the atp_matches_table represent our fact table and the other tables are the dimension tables that contains extra information needed to perform some queries. This model tries to keep a normalize and denormalize ratio in the sense that new queries can appear after this model decision have been take. Here you can see as well, that the lines shows the foreign keys relations that this schema keeps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "After we'd gather all the information from our raw databases it's necessary to perform the ETL -> transformation and load phase processes. \n",
    "1. In the last steps, we Extract the information from the databases\n",
    "2. Then, we understood the data behavior so we can make design decisions to make the respective Transformations\n",
    "3. Afterward, we remove some duplicate and missing data and select only those useful columns\n",
    "4. We needed to create a time table for our data model star schema, for this, each possible table had to have a timestamp format column depending on each value row. That's why the 'user-defined function (UDF) was created before loading the data.\n",
    "5. For each possible date, we create that timestamp column\n",
    "6. Next, we load a temp view table using CreateOrReplaceTempView spark function. This performed for each dimension table and the fact table as well.\n",
    "7. Then in the load step, we create the time table based on the timestamps in the fact table and some udf functions to obtain data such as day, week, month, year, and so on so forth.\n",
    "8. Lastly, we performed the respective quality checks that are well explained in its section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "toTimeStampFormat = udf(lambda x: pd.to_datetime(x,format=\"%Y%m%d\"),TimestampType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Select only the useful columns from our fact table (atp_matches) and save it to a spark temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_matches_valid = df_spark_matches.withColumn(\"ts_tourneyDate\", toTimeStampFormat(\"tourney_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_matches_valid.createOrReplaceTempView('atp_matches_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Select only the useful columns from our dimension table (atp_players) and save it to a spark temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid = df_spark_valid.withColumn(\"ts_bornDate\", toTimeStampFormat(\"bornDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid.createOrReplaceTempView('atp_players_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Select only the useful columns from our dimension table (atp_ranks) and save it to a spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid_ranks = df_spark_valid_ranks.withColumn(\"ts_rankingDate\", toTimeStampFormat(\"ranking_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_valid_ranks.createOrReplaceTempView('atp_ranks_tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Create time mandatory dimension (time_table) and save it to a spark temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "spark.udf.register(\"get_hour\", lambda x: int(x.hour))\n",
    "spark.udf.register(\"get_day\", lambda x: int(x.day))\n",
    "spark.udf.register(\"get_week\", lambda x: int(x.isocalendar()[1]))\n",
    "spark.udf.register(\"get_month\", lambda x: int(x.month))\n",
    "spark.udf.register(\"get_year\", lambda x: int(x.year))\n",
    "spark.udf.register(\"get_dayofweek\", lambda x: int(x.weekday()))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = spark.sql(\"\"\"\n",
    "SELECT distinct ts_tourneyDate as ts, get_day(ts_tourneyDate) as day, get_week(ts_tourneyDate) as week, get_month(ts_tourneyDate) as month, get_year(ts_tourneyDate) as year, get_dayofweek(ts_tourneyDate) as dayofweek\n",
    "from atp_matches_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+-----+----+---------+\n",
      "|                 ts|day|week|month|year|dayofweek|\n",
      "+-------------------+---+----+-----+----+---------+\n",
      "|2016-05-16 00:00:00| 16|  20|    5|2016|        0|\n",
      "|2012-02-27 00:00:00| 27|   9|    2|2012|        0|\n",
      "|2011-09-12 00:00:00| 12|  37|    9|2011|        0|\n",
      "|2008-12-01 00:00:00|  1|  49|   12|2008|        0|\n",
      "|2019-09-09 00:00:00|  9|  37|    9|2019|        0|\n",
      "+-------------------+---+----+-----+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this can take some time\n",
    "time_table.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.createOrReplaceTempView('time_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "- The quality checks performed include source/count completeness in each table, integrity constraints on the relational database such as datatypes verification and primary key and foreign key consistency, this was checked performing some SQL queries\n",
    "\n",
    "- Next you can find the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Verify source count check to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def checkCountItems(spark,table,quantity):\n",
    "    sql_table = spark.sql(f\"\"\"\n",
    "    SELECT count(*) from {table}\n",
    "    \"\"\")\n",
    "    amount = sql_table.first()[0]\n",
    "    if(amount<quantity):\n",
    "         raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "    else:\n",
    "        return f\"Data quality check sucess. {table} returned {amount} results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    Table atp_matches_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data quality check sucess. atp_matches_table returned 814534 results'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkCountItems(spark,'atp_matches_table',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    Table atp_players_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data quality check sucess. atp_players_table returned 42055 results'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkCountItems(spark,'atp_players_table',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    Table atp_ranks_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data quality check sucess. atp_ranks_tables returned 2509664 results'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkCountItems(spark,'atp_ranks_tables',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    Table time_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data quality check sucess. time_table returned 4063 results'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkCountItems(spark,'time_table',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Check datatypes of the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_column_types(spark_df):\n",
    "    \"\"\"Count number of columns per type\"\"\"\n",
    "    return pd.DataFrame(spark_df.dtypes).groupby(1, as_index=False)[0].agg({'count':'count', 'names': lambda x: \" | \".join(set(x))}).rename(columns={1:\"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>string</td>\n",
       "      <td>33</td>\n",
       "      <td>round | best_of | tourney_date | w_1stWon | ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>1</td>\n",
       "      <td>ts_tourneyDate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type  count                                              names\n",
       "0     string     33  round | best_of | tourney_date | w_1stWon | ma...\n",
       "1  timestamp      1                                     ts_tourneyDate"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_column_types(df_spark_matches_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int</td>\n",
       "      <td>2</td>\n",
       "      <td>bornDate | playerId</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>string</td>\n",
       "      <td>4</td>\n",
       "      <td>firstName | hand | lastName | country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>1</td>\n",
       "      <td>ts_bornDate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type  count                                  names\n",
       "0        int      2                    bornDate | playerId\n",
       "1     string      4  firstName | hand | lastName | country\n",
       "2  timestamp      1                            ts_bornDate"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_column_types(df_spark_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int</td>\n",
       "      <td>4</td>\n",
       "      <td>rank | ranking_date | points | player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>1</td>\n",
       "      <td>ts_rankingDate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type  count                                  names\n",
       "0        int      4  rank | ranking_date | points | player\n",
       "1  timestamp      1                         ts_rankingDate"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_column_types(df_spark_valid_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>string</td>\n",
       "      <td>5</td>\n",
       "      <td>year | day | month | week | dayofweek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type  count                                  names\n",
       "0     string      5  year | day | month | week | dayofweek\n",
       "1  timestamp      1                                     ts"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_column_types(time_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Perfom some join queries to check primary and foreign keys integrity\n",
    "\n",
    "      Get the Tourney_ID, Tourney_date, Tourney_name, level, score, name of the player, hand and origin country of all those players that its origin country is Colombia (COL) order desc by player's last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "Select ATPm.tourney_id, ATPm.ts_tourneyDate, ATPm.tourney_name, ATPm.tourney_level, ATPm.score, ATPp.firstName, ATPp.lastName, ATPp.hand, ATPp.country\n",
    "from atp_matches_table ATPm\n",
    "join atp_players_table ATPp\n",
    "on ATPm.winner_id=ATPp.playerId\n",
    "where(ATPp.country=\"COL\") order by ATPp.lastName DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------+-------------+---------------+---------+---------------+----+-------+\n",
      "|          tourney_id|     ts_tourneyDate|tourney_name|tourney_level|          score|firstName|       lastName|hand|country|\n",
      "+--------------------+-------------------+------------+-------------+---------------+---------+---------------+----+-------+\n",
      "|2012-M-FU-COL-04A...|2012-10-01 00:00:00| Colombia F4|            S|        6-1 6-0| Barlaham|Zuluaga Gaviria|   R|    COL|\n",
      "|2016-M-FU-COL-08A...|2016-11-21 00:00:00| Colombia F8|            S|    1-6 6-2 6-2| Barlaham|Zuluaga Gaviria|   R|    COL|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|    1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|\n",
      "|2014-M-FU-COL-06A...|2014-09-15 00:00:00| Colombia F6|            S|     7-6(5) 6-4|  Steffen|        Zornosa|   L|    COL|\n",
      "|2015-M-FU-COL-09A...|2015-11-02 00:00:00| Colombia F9|            S|4-6 6-3 5-2 RET|  Steffen|        Zornosa|   L|    COL|\n",
      "|2013-M-FU-BOL-05A...|2013-10-07 00:00:00|  Bolivia F5|            S|        6-0 6-1|  Steffen|        Zornosa|   L|    COL|\n",
      "|2015-M-FU-COL-09A...|2015-11-02 00:00:00| Colombia F9|            S|        6-1 6-4|  Steffen|        Zornosa|   L|    COL|\n",
      "|2014-M-FU-MEX-06A...|2014-06-16 00:00:00|   Mexico F6|            S|     7-6(4) 6-1|  Steffen|        Zornosa|   L|    COL|\n",
      "|2014-M-FU-COL-05A...|2014-09-08 00:00:00| Colombia F5|            S|        6-0 6-4|  Steffen|        Zornosa|   L|    COL|\n",
      "|2015-M-FU-COL-04A...|2015-06-29 00:00:00| Colombia F4|            S|        6-3 6-0|  Steffen|        Zornosa|   L|    COL|\n",
      "|2012-M-FU-COL-03A...|2012-09-24 00:00:00| Colombia F3|            S|     7-6(7) 6-2|  Steffen|        Zornosa|   L|    COL|\n",
      "|2014-M-FU-MEX-05A...|2014-05-19 00:00:00|   Mexico F5|            S|    6-3 5-7 6-4|  Steffen|        Zornosa|   L|    COL|\n",
      "|2011-M-FU-COL-03A...|2011-08-15 00:00:00| Colombia F3|            S|    6-3 1-6 6-4|  Steffen|        Zornosa|   L|    COL|\n",
      "|2015-M-FU-COL-08A...|2015-10-26 00:00:00| Colombia F8|            S|        6-2 6-0|  Steffen|        Zornosa|   L|    COL|\n",
      "|2013-M-FU-BOL-05A...|2013-10-07 00:00:00|  Bolivia F5|            S|        7-5 6-2|  Steffen|        Zornosa|   L|    COL|\n",
      "|2013-M-FU-COL-06A...|2013-11-11 00:00:00| Colombia F6|            S|        7-5 7-5|  Steffen|        Zornosa|   L|    COL|\n",
      "|2015-M-FU-COL-04A...|2015-06-29 00:00:00| Colombia F4|            S|        6-3 6-3|  Steffen|        Zornosa|   L|    COL|\n",
      "|2015-M-FU-COL-06A...|2015-07-27 00:00:00| Colombia F6|            S|        6-0 6-3|  Steffen|        Zornosa|   L|    COL|\n",
      "|2011-M-FU-VEN-01A...|2011-04-25 00:00:00|Venezuela F1|            S|    6-2 4-6 6-4|  Steffen|        Zornosa|   L|    COL|\n",
      "|2011-M-FU-VEN-05A...|2011-06-20 00:00:00|Venezuela F5|            S|        6-2 6-2|  Steffen|        Zornosa|   L|    COL|\n",
      "+--------------------+-------------------+------------+-------------+---------------+---------+---------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show(20, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6142"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "      Get the Tourney_ID, Tourney_date, Tourney_name, level, score, name of the player, hand, origin country, ranking and ranking date of all those players that its origin country is Colombia (COL) order desc by player's last name\n",
    "      \n",
    "       Here we have to take into account that the join performed is neither right or left join so that's why we got more tuples, the rank and ranking_date is repeated in all the possible records. This was just and example to show how we can use the ranking table as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query2 = spark.sql(\"\"\"\n",
    "Select ATPm.tourney_id, ATPm.ts_tourneyDate, ATPm.tourney_name, ATPm.tourney_level, ATPm.score, ATPp.firstName, ATPp.lastName, ATPp.hand, ATPp.country, ATPr.rank, ATPr.ranking_date\n",
    "from atp_matches_table ATPm\n",
    "join atp_players_table ATPp\n",
    "on ATPm.winner_id=ATPp.playerId\n",
    "join atp_ranks_tables ATPr\n",
    "on ATPp.playerId=ATPr.player\n",
    "where(ATPp.country=\"COL\") order by ATPp.lastName DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------+-------------+-----------+---------+---------------+----+-------+----+------------+\n",
      "|          tourney_id|     ts_tourneyDate|tourney_name|tourney_level|      score|firstName|       lastName|hand|country|rank|ranking_date|\n",
      "+--------------------+-------------------+------------+-------------+-----------+---------+---------------+----+-------+----+------------+\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1668|    20130225|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1683|    20121224|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1672|    20130218|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1655|    20121119|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1681|    20121217|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1683|    20130114|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1671|    20130211|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1665|    20121022|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1671|    20121112|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1665|    20121203|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1676|    20121210|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1683|    20121231|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1684|    20130107|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1679|    20130128|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1674|    20130204|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1668|    20121015|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1676|    20130318|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1665|    20121029|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1666|    20121105|\n",
      "|2013-M-FU-COL-04A...|2013-08-12 00:00:00| Colombia F4|            S|1-6 6-4 6-3| Barlaham|Zuluaga Gaviria|   R|    COL|1652|    20121126|\n",
      "+--------------------+-------------------+------------+-------------+-----------+---------+---------------+----+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2.show(20, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3305167"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "- Is possible to find the data dictionary in the next local file path: atpTennisData/tennis_atp/matches_data_dictionary.txt. Or in this Github file: https://github.com/JeffSackmann/tennis_atp/blob/master/matches_data_dictionary.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "The tools and technologies used in this project were mainly Spark and Python, though the raw data and spark host can be achive using AWS services such as EMR, S3 buckets and Redshift.\n",
    "\n",
    "So to sum up the reasons of choosing these techonologies are:\n",
    "- Easily scale up using the previous mention techonology\n",
    "- Rapidly query, analyze, and transform data at scale\n",
    "- Can handle big amount of data, in this project we worked with:\n",
    "    - 814.534 Matches\n",
    "    - 42.055 Players\n",
    "    - 2.509.664 Ranks\n",
    "    - 4.063 Time data\n",
    "    - In total: 3.370.316 data tuples with 6 attributes in average each tuple. So this's around 20.221.896 raw data loaded in a Apache Spark\n",
    "- Great perfomance in simple and complex queries\n",
    "- Is possible to handle partitions so the queries and processing can be even faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Propose how often the data should be updated and why.\n",
    "- This dataset keeps the information files per year for the matches and per 10 years for the rankings, though each year new tournaments, matches, rankings, and players can be appended to the database. Usually, this kind of sports run by 6 months cycles A.K.A seasons that approx. have a length of 6 months. So, my suggestion could be to update the data each season iterative cuz the information of 2019-2nd season will be not updated by the 2020-1st season. So the pipeline to create, let's say in Airflow, will be performed every 6 months, can be performed each month if it's required, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The data was increased by 100x.\n",
    "- In this case, it will be necessary to evaluate mainly the architecture that Spark is running in. So the first step will be to increase the nodes in which the workers perform the different tasks, It's not possible to know exactly how much nodes will be needed for the 2B (2M*100x) data, with this in mind, it is necessary to perform some trial and error to fix the system architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "- Taking into account the data structure of the raw files in this project, it's necessary to append the new information in the current year file with the proper timestamp, so when we execute the Airflow pipeline with a @daily schedule, it only gathers the new information according to the previous day. With this in mind, those big created queries need to allocate more machine resources to perform better all the days before the business needs the dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The database needed to be accessed by 100+ people\n",
    "- If the database is only used for reading purposes and in a non-update data window time it should be not many problems regarding ACID properties. Nevertheless, if more than 100+ people access just for reading purposes, the master node or driver needs to have more capacity of scheduling tasks to its works, as the driver increases so the workers.\n",
    "- The more people accessing the database the more cpu resources you need to get a fast experience. By using a distributed database you can to improve your replications and partitioning to get faster query results for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
